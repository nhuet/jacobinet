{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c3e1fa-85cd-4595-9028-1ac1186f2430",
   "metadata": {},
   "source": [
    "# Tutorial: Implementing Custom Backward Pass for Non-Native Keras Operators with Jacobinet\n",
    "\n",
    "## Introduction\n",
    "This tutorial demonstrates how to implement custom backward passes for non-native Keras operators using Jacobinet. We'll explore two types of backward layers:\n",
    "\n",
    "- BackwardLinearLayer: For layers with a constant partial derivative.\n",
    "- BackwardNonLinearLayer: For layers where the partial derivative depends on the input.\n",
    "\n",
    "## Overview of Custom Operators:\n",
    "- PlusConstant: A linear layer that adds a constant to the input.\n",
    "- Clip: A non-linear layer that clips input values to a specified range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08d9dc-2aeb-4ec4-b95c-4abf07ba5883",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b406c84-2afa-4454-b754-2d9a3dc9f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c924c1-e2a7-446f-97af-b0352cf24abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb18db-1ee2-447a-b77a-9faa41447027",
   "metadata": {},
   "source": [
    "## 1. Defining the Custom Operator: PlusConstant\n",
    "We start by creating a custom Keras layer that adds a constant value to its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf2ed0-e40d-4b27-9c6d-79860145d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Layer\n",
    "\n",
    "\n",
    "class PlusConstant(Layer):\n",
    "    def __init__(self, constant, **kwargs):\n",
    "        super(PlusConstant, self).__init__(**kwargs)\n",
    "        self.constant = keras.ops.convert_to_tensor(constant)\n",
    "\n",
    "    def call(self, inputs_):\n",
    "        return inputs_ + self.constant\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"constant\": keras.saving.serialize_keras_object(self.constant)})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        constant_config = config.pop(\"constant\")\n",
    "        constant = keras.saving.deserialize_keras_object(constant_config)\n",
    "        return cls(constant=constant, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304b08c-797b-4ddb-8e5e-17a1c48d6f7a",
   "metadata": {},
   "source": [
    "### Using PlusConstant in a Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78834402-e340-4f41-bc9c-a7b820844551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Sequential\n",
    "\n",
    "layers = [Input((10,)), Dense(2), PlusConstant(2), Activation(\"sigmoid\"), Dense(1)]\n",
    "model_plusconstant = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f4856-3354-4e1d-b2d1-68c4e1d02a08",
   "metadata": {},
   "source": [
    "If we try to build a Jacobinet backward model directly, it will fail because PlusConstant is not a native Keras layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea548d64-b353-4358-8392-6040e467a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobinet import clone_to_backward\n",
    "\n",
    "try:\n",
    "    backward_model = clone_to_backward(model_plusconstant)\n",
    "except ValueError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81cf660-a45e-4523-9904-ec30eb345013",
   "metadata": {},
   "source": [
    "# 2. Creating BackwardLinearLayer for PlusConstant\n",
    "\n",
    "Derivative Explanation:\n",
    "Since $\\frac{\\partial \\text{PlusConstant}(x)}{\\partial x}= 1$ the gradient is constant.\n",
    "\n",
    "Then its backward mapping will be what we denote a BackwardLinearLayer: a backward layer that outputs constant value.\n",
    "Among existing implemented BackwardLinear layers, we can enumerate Dense(activation=None/linear), Conv(activation=None/linear), AveragePooling, BacthNormalization, Cropping, Padding ...\n",
    "\n",
    "We only need to override a routine of the call function denoted \n",
    "\n",
    "```python\n",
    "def call_on_reshaped_gradient(\n",
    "        self, gradient, input=None, training=None, mask=None\n",
    "    ):\n",
    "```\n",
    "\n",
    "this function tales as input the current jacobian (*gradient*) propagated from the output of the neural network\n",
    "and the input of the layer. This input is not used to compute the backward pass over a BackwardLinear Layer.\n",
    "\n",
    "## Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93b144-53bd-4f2e-a1ea-92e40f6abdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobinet.layers.layer import BackwardLinearLayer\n",
    "\n",
    "\n",
    "class BackwardPlusConstant(BackwardLinearLayer):\n",
    "    def call_on_reshaped_gradient(self, gradient, input=None, training=None, mask=None):\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327b43f-6d02-4687-9cf7-7e4f38167a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_keras2backward_classes = {PlusConstant: BackwardPlusConstant}\n",
    "backward_model_plusconst = clone_to_backward(\n",
    "    model=model_plusconstant,\n",
    "    mapping_keras2backward_classes=mapping_keras2backward_classes,\n",
    "    gradient=keras.Variable(np.ones((1, 1))),\n",
    ")\n",
    "\n",
    "# Testing with random input\n",
    "random_input = np.random.rand(10)\n",
    "grad_v0 = backward_model_plusconst(random_input[None])\n",
    "print(grad_v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c32b5-abfd-4797-ae7f-d98725a5edd1",
   "metadata": {},
   "source": [
    "# 3. Enhancing with LayerBackward Property (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a48d5-8f6f-416e-913d-bc89fbc27d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Identity, Input\n",
    "\n",
    "\n",
    "class BackwardPlusConstant_withLayerBackward(BackwardLinearLayer):\n",
    "    def __init__(self, layer: PlusConstant, **kwargs):\n",
    "        super().__init__(layer=layer, **kwargs)\n",
    "        self.layer_backward = Identity()\n",
    "        self.layer_backward(Input(self.output_dim_wo_batch))\n",
    "\n",
    "\n",
    "mapping_keras2backward_classes = {PlusConstant: BackwardPlusConstant_withLayerBackward}\n",
    "backward_model_plusconst_v2 = clone_to_backward(\n",
    "    model=model_plusconstant,\n",
    "    mapping_keras2backward_classes=mapping_keras2backward_classes,\n",
    "    gradient=keras.Variable(np.ones((1, 1))),\n",
    ")\n",
    "\n",
    "grad_v2 = backward_model_plusconst_v2(random_input[None])\n",
    "print(grad_v0 == grad_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b1a12-9d6d-4baa-8586-4726d929c114",
   "metadata": {},
   "source": [
    "# 4. Defining the Custom Operator: Clip\n",
    "\n",
    "The Clip operator limits inputs to a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75397e53-28d4-491e-aacf-76a565ca7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(Layer):\n",
    "    def __init__(self, vmin=0, vmax=1, **kwargs):\n",
    "        super(Clip, self).__init__(**kwargs)\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "\n",
    "    def call(self, inputs_):\n",
    "        return keras.ops.clip(inputs_, self.vmin, self.vmax)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"vmin\": self.vmin, \"vmax\": self.vmax})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2ded5-f0b0-4a88-ba7a-18e6a88c461e",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e463baf-1cab-4cf7-ac70-d0a747d5e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_clip = Clip(0, 2)\n",
    "print(layer_clip(np.ones((1, 1))))  # Output: 1\n",
    "print(layer_clip(-np.ones((1, 1))))  # Output: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba44990-aab5-4fe7-870a-6f5a57591065",
   "metadata": {},
   "source": [
    "# 5. Creating BackwardNonLinearLayer for Clip\n",
    "\n",
    "Derivative Explanation:\n",
    "Derivative is 1 if $v_{min}\\leq x \\leq v_max$, otherwise 0.\n",
    "\n",
    "But to compute its derivative we need to know the value of its input. We use another BackwardLayer designed for non linear operators:\n",
    "BackwardNonLinearLayer\n",
    "\n",
    "## Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c0174-8580-4ba5-a47a-9038a95cc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Input((10,)), Dense(2), layer_clip, Activation(\"sigmoid\"), Dense(1)]\n",
    "model_clip = Sequential(layers)\n",
    "\n",
    "from jacobinet.layers.layer import BackwardNonLinearLayer\n",
    "\n",
    "\n",
    "class BackwardClip(BackwardNonLinearLayer):\n",
    "\n",
    "    def call_on_reshaped_gradient(self, gradient, input=None, training=None, mask=None):\n",
    "        eps = keras.config.epsilon()\n",
    "        mask_lower_vmax = keras.ops.sign(self.layer.vmax - input - eps)  # 1 iff input <= vmax\n",
    "        mask_lower_vmin = keras.ops.sign(input - self.layer.vmin + eps)  # 1 iff input >= vmax\n",
    "\n",
    "        mask = mask_lower_vmax * mask_lower_vmin  # 1 iff x in [vmin, vmax], 0 else\n",
    "        return gradient * mask\n",
    "\n",
    "\n",
    "mapping_keras2backward_classes = {Clip: BackwardClip}\n",
    "backward_model_clip = clone_to_backward(\n",
    "    model=model_clip,\n",
    "    mapping_keras2backward_classes=mapping_keras2backward_classes,\n",
    "    gradient=keras.Variable(np.ones((1, 1))),\n",
    ")\n",
    "\n",
    "grad_v3 = backward_model_clip(random_input[None])\n",
    "print(grad_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5034ead-8015-48f1-8bda-c1e76088590c",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "In this tutorial, we demonstrated how to implement custom backward layers using Jacobinet for non-native Keras operators:\n",
    "\n",
    "- PlusConstant as an example of a linear backward layer.\n",
    " \n",
    "- Clip as an example of a non-linear backward layer.\n",
    "                                                                                         \n",
    "These techniques extend *Jacobinet*â€™s utility to handle custom operators, improving flexibility for neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6bef6-c44a-4070-b97a-26b1f503a7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
