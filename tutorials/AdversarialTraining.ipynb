{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061d19d8-022d-4b2c-a81e-afd0adbe7026",
   "metadata": {},
   "source": [
    "# Robust Training with Jacobinet and Adversarial Attacks\n",
    "\n",
    "This tutorial demonstrates the use of Jacobinet for robust training in neural networks. \n",
    "Jacobinet allows the backward pass of a neural network to be represented as a neural network with shared weights. \n",
    "\n",
    "**Goals:**\n",
    "- Understand adversarial attacks (FGSM, PGD) and their impact on model robustness.\n",
    "- Use Jacobinet to implement robust training by regularizing against adversarial examples.\n",
    "- Evaluate robustness with AutoAttack for both baseline and robust training.\n",
    "\n",
    "We will:\n",
    "1. Train a baseline model and evaluate its adversarial robustness.\n",
    "2. Train a robust model with adversarial regularization using Jacobinet.\n",
    "3. Compare adversarial success rates for both models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd4c645-09ce-45d5-8076-5717173444e5",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759689f7-e1cc-4d59-b730-566d465a895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"torchattacks\"\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    !{sys.executable} -m pip install \"matplotlib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6889e1-1075-4728-8cc2-2a4dad3e0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e9c5e-41db-4741-9b8b-cb8c07044f54",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "We will use the MNIST dataset for this tutorial. The dataset is normalized to the [0, 1] range and reshaped for compatibility with the convolutional model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9b89b-2e2a-4847-95da-3ec3fc7b44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the MNIST data and split it into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale the images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape images to have an additional channel dimension (1, 28, 28)\n",
    "x_train = np.expand_dims(x_train, 1)\n",
    "x_test = np.expand_dims(x_test, 1)\n",
    "\n",
    "# Convert class labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beafcc44-3d81-4d34-836a-a8aa5e0b089c",
   "metadata": {},
   "source": [
    "## Define and Train the Baseline Model\n",
    "\n",
    "We will build a simple Convolutional Neural Network (CNN) using Keras to serve as the baseline model. \n",
    "This model will be trained on MNIST and evaluated for accuracy on clean data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52589ad-076d-4274-98a4-0bb0e013c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers\n",
    "\n",
    "# Define the model architecture\n",
    "\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(1, 28, 28)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        # layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        # layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_model = Sequential(model.layers + [layers.Activation(\"softmax\")])\n",
    "\n",
    "train_model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_model.fit(x_train, y_train, batch_size=128, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ac9dd-b968-4940-aa8b-94542beb8541",
   "metadata": {},
   "source": [
    "## Evaluate Robustness of Baseline Model\n",
    "\n",
    "We use AutoAttack, a strong adversarial attack framework, to test the baseline model's robustness. \n",
    "AutoAttack generates adversarial examples by varying the attack radius (`epsilon`), and we measure the model's accuracy on these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bafe0f-20e8-4313-95d5-38ad7e24cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchattacks\n",
    "\n",
    "# Test robustness at different epsilon values\n",
    "n = 200\n",
    "random_index = np.random.permutation(len(x_test))[:n]\n",
    "adv_acc = []\n",
    "eps_values = [np.round(eps_i, 2) for eps_i in np.linspace(0.01, 0.2, 10)]\n",
    "for eps in eps_values:\n",
    "for i in range(100):\n",
    "    random_index = np.arange(100) + i * 100\n",
    "    auto_attack = torchattacks.attacks.autoattack.AutoAttack(model, eps=eps_values)\n",
    "    adv_data = auto_attack(\n",
    "        torch.Tensor(x_test[random_index]), torch.tensor(y_test[random_index].argmax(-1))\n",
    "    )\n",
    "    acc = (\n",
    "        len(\n",
    "            np.where(\n",
    "                model.predict(adv_data, verbose=0).argmax(-1) != y_test[random_index].argmax(-1)\n",
    "            )[0]\n",
    "        )\n",
    "        / len(random_index)\n",
    "        * 100\n",
    "    )\n",
    "    if len(adv_acc):\n",
    "        adv_acc.append(max(adv_acc[-1], acc))\n",
    "    else:\n",
    "        adv_acc.append(acc)\n",
    "\n",
    "print(acc)\n",
    "\n",
    "plt.plot(eps_values, adv_acc)\n",
    "plt.title(\"Distribution of adversarial success rates with baseline training\")\n",
    "plt.xlabel(\"Epsilon (attack radius)\")\n",
    "plt.ylabel(\"Adversarial success rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da59ad-3f43-442e-a72a-b6cc4b1512b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.unique(np.abs((adv_data.cpu().detach().numpy() - x_test[random_index]).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528174e-69c4-4686-a14e-4c3e5bfeacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1e1fd-8f07-499f-a0e2-02160bf6d913",
   "metadata": {},
   "source": [
    "## Robust Training with Jacobinet\n",
    "\n",
    "To improve robustness, we will train a model that outputs predictions for both clean and adversarial examples. \n",
    "Jacobinet is used to create adversarial examples with Projected Gradient Descent (PGD), which are integrated into the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cd14a-69fb-49cc-a3e2-e5bc1f406b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobinet.attacks import get_adv_model\n",
    "\n",
    "pgd_model = get_adv_model(\n",
    "    model, loss=\"logits\", epsilon=1e-2, attack=\"pgd\", n_iter=20\n",
    ")  # think of clipping\n",
    "\n",
    "x = layers.Input(shape=(1, 28, 28))\n",
    "y = layers.Input((10,))\n",
    "\n",
    "model_adv = keras.models.Model([x, y], [model(x), model(pgd_model([x, y]))])\n",
    "model_adv.compile(\n",
    "    \"adam\",\n",
    "    loss=[\n",
    "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    ],\n",
    "    metrics=[\"accuracy\", \"accuracy\"],\n",
    "    loss_weights=[1, 100],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a043b14-1f4c-4ea5-b54e-c15e35dd84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adv.fit(\n",
    "    [x_train, y_train],\n",
    "    [y_train, y_train],\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882cd4b-a673-45dd-83eb-be540d8361c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchattacks\n",
    "\n",
    "auto_attack = torchattacks.attacks.autoattack.AutoAttack(model, eps=0.2)\n",
    "acc_batch = []\n",
    "for i in range(10):\n",
    "    batch = i + np.arange(100)\n",
    "    adv_data_robust = auto_attack(\n",
    "        torch.Tensor(x_test[batch]), torch.tensor(y_test[batch].argmax(-1))\n",
    "    )\n",
    "    acc = (\n",
    "        len(\n",
    "            np.where(\n",
    "                model.predict(adv_data_robust, verbose=0).argmax(-1) != y_test[batch].argmax(-1)\n",
    "            )[0]\n",
    "        )\n",
    "        / len(batch)\n",
    "        * 100\n",
    "    )\n",
    "    print(acc)\n",
    "    acc_batch.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db32bf6-be6b-4d21-8bc6-7220ccef6e1e",
   "metadata": {},
   "source": [
    "## Evaluate Robustness of Robust Model\n",
    "\n",
    "We use AutoAttack again to evaluate the robust model under varying attack radii. \n",
    "This allows us to compare the adversarial success rates of the baseline and robust models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe760126-4d4d-4d0a-9616-a55d5aba2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_acc_robust = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    print(eps)\n",
    "    auto_attack = torchattacks.attacks.autoattack.AutoAttack(model, eps=eps)\n",
    "    adv_data_robust = auto_attack(\n",
    "        torch.Tensor(x_test[random_index]), torch.tensor(y_test[random_index].argmax(-1))\n",
    "    )\n",
    "    acc = (\n",
    "        len(\n",
    "            np.where(\n",
    "                model.predict(adv_data_robust, verbose=0).argmax(-1)\n",
    "                != y_test[random_index].argmax(-1)\n",
    "            )[0]\n",
    "        )\n",
    "        / len(random_index)\n",
    "        * 100\n",
    "    )\n",
    "    adv_acc_robust.append(acc)\n",
    "\n",
    "plt.plot(eps_values, adv_acc, label=\"Baseline Training\")\n",
    "plt.plot(eps_values, adv_acc_robust, label=\"Robust Training\")\n",
    "plt.title(\"Adversarial Success Rates: Baseline vs. Robust Training\")\n",
    "plt.xlabel(\"Epsilon (attack radius)\")\n",
    "plt.ylabel(\"Adversarial success rate\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841d6fa-43e3-44e9-b07a-d83b9557a4c3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the use of Jacobinet for robust training against adversarial attacks. Key takeaways include:\n",
    "1. Baseline models are vulnerable to adversarial examples, as shown by the high adversarial success rates.\n",
    "2. Robust training with Jacobinet significantly improves resistance to adversarial attacks.\n",
    "3. This workflow can be extended to other datasets and adversarial attack frameworks.\n",
    "\n",
    "Jacobinet's ability to treat the backward pass as a neural network opens exciting possibilities for research in robustness and adversarial machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8c05a-eee8-4434-9cee-7d219078f0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
