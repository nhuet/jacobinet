{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03caa8d7-1fba-4db4-8474-83ff6d9b93f1",
   "metadata": {},
   "source": [
    "# Estimating the Local Lipschitz Constant of a Neural Network Using Jacobinet\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, we will estimate the local Lipschitz constant of a neural network using the Jacobian matrix and explore how this constant relates to the network's robustness. A neural network's Lipschitz constant bounds the rate at which its outputs can change with respect to small input perturbations. Understanding and controlling this constant is critical for:\n",
    "\n",
    "- Adversarial robustness: Ensuring the network resists small, intentional perturbations.\n",
    "- Stability: Preventing large output changes due to minor input variations.\n",
    "- Generalization: Improving the network's performance on unseen data.\n",
    "\n",
    "We will use the *Jacobinet* library (based on Keras) to calculate the Jacobian and maximize the $L_p$ norm of the gradient \n",
    "to estimate the Lipschitz constant. This provides a lower bound for the Lipschitz constant, a key metric in robustness evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193309e-ecfc-48f8-b970-5a7f8cfd9d53",
   "metadata": {},
   "source": [
    "- When running this notebook on Colab, we need to install *decomon* if on Colab. \n",
    "- If you run this notebook locally, do it inside the environment in which you [installed *jacobinet*](https://ducoffeM.github.io/jacobinet/main/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405f385-7b08-41c8-b0d1-6ed16e16147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Colab: install the library\n",
    "on_colab = \"google.colab\" in str(get_ipython())\n",
    "if on_colab:\n",
    "    import sys  # noqa: avoid having this import removed by pycln\n",
    "\n",
    "    # install dev version for dev doc, or release version for release doc\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install git+https://github.com/ducoffeM/jacobinet@main#egg=decomon\n",
    "    # install desired backend (by default torch)\n",
    "    !{sys.executable} -m pip install \"torch\"\n",
    "    !{sys.executable} -m pip install \"keras\"\n",
    "\n",
    "    # extra librabry used in this notebook\n",
    "    !{sys.executable} -m pip install \"numpy\"\n",
    "    # missing imports IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7dc89-bfa7-4062-a88a-02c032b5726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this environment variable *before* importing torch, otherwise it has no effect.\n",
    "# Ideally, we'd only set this if torch.backends.mps.is_available() is True,\n",
    "# but checking that requires importing torch first, which would make this setting too late.\n",
    "# So we preemptively enable the MPS fallback just in case MPS is available.\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9d1bc-cd94-4598-9317-56a8d7fc5073",
   "metadata": {},
   "source": [
    "# 1. Building the Neural Network\n",
    "We start by defining a simple feedforward neural network with two dense layers and a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32862aee-ab9e-4710-a266-27ebef4b6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    input_ = Input((10,))\n",
    "    x = Dense(10, name=\"Dense1\")(input_)\n",
    "    x = Activation(\"relu\", name=\"ReLU1\")(x)\n",
    "    x = Dense(10)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    output = Dense(2, name=\"Output\")(x)\n",
    "    return Model(input_, output)\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ccd29-e091-4ca1-af1f-a80120af8344",
   "metadata": {},
   "source": [
    "## 2. Computing the Jacobian with Jacobinet\n",
    "\n",
    "We will now compute the Jacobian matrix using Jacobinet’s get_backward_model function. \n",
    "This model returns the gradient of each output with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b10cb8-1e66-4fc9-a876-d1bb4a28d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jacobinet\n",
    "import numpy as np\n",
    "from jacobinet import clone_to_backward\n",
    "\n",
    "# Placeholder gradient to compute the Jacobian\n",
    "gradient_placeholder = keras.Variable(np.ones((1, 2), dtype=\"float32\"))\n",
    "\n",
    "# Compute backward model for Jacobian calculation\n",
    "backward_model = clone_to_backward(model, gradient=gradient_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ded535-fbdc-4ec7-9264-91e1c1f93d37",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The Jacobian represents the gradients of the output w.r.t. the input.\n",
    "get_backward_model builds a model to compute these gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e569615-31c7-4f01-84f1-d3267579f020",
   "metadata": {},
   "source": [
    "# 3. Estimating the Lipschitz Constant\n",
    "To compute the Lipschitz constant, we use the L2 norm (p=2) of the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e0705-cb76-4119-8806-eb96771cecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jacobinet import get_lipschitz_model\n",
    "\n",
    "# Create a Lipschitz model using the L2 norm (p=2)\n",
    "lipschitz_model = get_lipschitz_model(backward_model, p=2)\n",
    "lipschitz_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00591cc5-212d-4cae-aa52-d42b41819de0",
   "metadata": {},
   "source": [
    "## Visualizing the Model Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42523bc3-fec0-4373-9226-65243e321b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils\n",
    "from IPython.display import HTML\n",
    "\n",
    "dot_img_file_lipschitz = \"./model_dense_lipschitz.png\"\n",
    "keras.utils.plot_model(\n",
    "    lipschitz_model, to_file=dot_img_file_lipschitz, show_shapes=True, show_layer_names=True\n",
    ")\n",
    "HTML(\n",
    "    '<div style=\"text-align: center;\"><img src=\"{}\" width=\"400\"/></div>'.format(\n",
    "        dot_img_file_lipschitz\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed68ea-1c98-45ad-8509-a0191209bb79",
   "metadata": {},
   "source": [
    "# 4. Evaluating the Lipschitz Constant on Random Data\n",
    "We now evaluate the Lipschitz constant using random input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1266f-272b-4e00-81bb-a7d7cdf883b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(np.random.rand(10)[None], dtype=\"float32\")  # Generate random input data\n",
    "\n",
    "# Compute the lower bound of the Lipschitz constant\n",
    "lipschitz_constant = lipschitz_model(data)\n",
    "print(f\"The Lipschitz constant is at least: {lipschitz_constant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1fac6-4294-467e-b6d8-9cfbb35e7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lipschitz_threshold = lipschitz_constant[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc9b9d-a47d-4d48-8041-f5522bfd58d5",
   "metadata": {},
   "source": [
    "# 5. Maximizing the Lp Norm with Adversarial Attacks (PGD)\n",
    "We use Projected Gradient Descent (PGD) to iteratively perturb the input and maximize the Lp norm, tightening the lower bound of the Lipschitz constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20e7f3-17e2-4c04-a1fe-553487c5cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchattacks\n",
    "\n",
    "\n",
    "class LipAttack(nn.Module):\n",
    "    def __init__(self, keras_model):\n",
    "        super().__init__()\n",
    "        self.keras_model = keras_model\n",
    "        self.lipschitz_threshold = keras.Variable(lipschitz_threshold.cpu().detach().numpy())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.keras_model(x)\n",
    "        return torch.cat(\n",
    "            [\n",
    "                keras.ops.relu(self.lipschitz_threshold - x),\n",
    "                keras.ops.relu(x - self.lipschitz_threshold),\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "\n",
    "# Wrap lipschitz_model with the attack class\n",
    "torch_lip_model = LipAttack(lipschitz_model)\n",
    "\n",
    "# Apply PGD attack for different iteration steps\n",
    "adv_data = data\n",
    "for steps in [10, 20, 40, 100, 200, 500, 1000]:\n",
    "    lip_attack = torchattacks.PGD(torch_lip_model, eps=10.0, steps=steps)\n",
    "    adv_data_ = lip_attack(torch.Tensor(adv_data), torch.Tensor([1, 0]))\n",
    "    lipschitz_constant_adv = lipschitz_model(adv_data)\n",
    "\n",
    "    if torch_lip_model(adv_data_).argmax().cpu().detach().numpy() == 1:\n",
    "        adv_data = adv_data_\n",
    "        torch_lip_model.lipschitz_threshold.assign(lipschitz_constant_adv[0].cpu().detach().numpy())\n",
    "    print(f\"Lipschitz constant after {steps} PGD steps: {lipschitz_constant_adv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c7650-b23d-457d-b0b2-8ab4bb00cee6",
   "metadata": {},
   "source": [
    "## 6. Theoretical Insights: Why Lipschitz Constant Matters\n",
    "\n",
    "### 1. Adversarial Robustness\n",
    "A smaller Lipschitz constant implies that the network’s output changes less when small perturbations are applied to the input. This makes it more resistant to **adversarial attacks**, where maliciously crafted input perturbations attempt to mislead the model.\n",
    "\n",
    "### 2. Stability and Generalization\n",
    "Networks with lower Lipschitz constants tend to generalize better, as they are less sensitive to noise or variations in input data. This also enhances training stability, as it prevents excessive variations in gradients.\n",
    "\n",
    "### 3. Mathematical Context\n",
    "The Lipschitz constant \\( L \\) is formally defined as:\n",
    "\n",
    "\n",
    "$$L = \\sup_{x \\neq y} \\frac{\\|f(x) - f(y)\\|}{\\|x - y\\|} $$\n",
    "\n",
    "\n",
    "Locally, this can be approximated using the **Jacobian matrix** \\( J(x) \\), which contains all first-order partial derivatives of the network’s outputs with respect to its inputs:\n",
    "\n",
    "$$ L = \\max_{x} \\|J(x)\\|_p $$\n",
    "\n",
    "\n",
    "where \\( \\|J(x)\\|_p \\) is the **Lp norm** of the Jacobian matrix. Maximizing this norm provides a lower bound on the Lipschitz constant, a key metric for evaluating the robustness of the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1477cb2-48ec-46ba-a8bd-c8188dce6c44",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial, we used Jacobinet to compute a lower bound for the Lipschitz constant by maximizing the Lp norm using adversarial attacks. The Lipschitz constant is a key measure of a network's robustness and generalization capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
